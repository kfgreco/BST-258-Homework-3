---
title: "Problem Set #3"
subtitle: "BST 258: Causal Inference -- Theory and Practice"
author: "Kimberly Greco"
date: ""
format:
  pdf:
    documentclass: scrartcl
    papersize: letter
    fontsize: 11pt
    geometry:
      - margin=1in
      - heightrounded
    number-sections: false
    colorlinks: true
    link-citations: true
    callout-appearance: simple
    callout-icon: false
    # figure options
    fig-width: 6
    fig-asp: 0.618
    fig-cap-location: bottom
    # code block options
    code-line-numbers: false
    code-block-bg: false
    highlight-style: nord
bibliography: refs.bib
---

```{r}
#| echo: false
#| message: false
#| label: global-setup
library(here)
```

\begin{center}
\href{https://github.com/kfgreco/BST-258-Homework-3}{Link to GitHub Repository}
\end{center}

{{< pagebreak >}}


# Deriving Efficient Influence Functions

A major goal of this assignment is to introduce some techniques for derivation of the efficient influence function (EIF) of regular asymptotically linear (RAL) estimators of a given statistical functional (target parameter). In the nonparametric statistical model, the EIF for the class of RAL estimators is unique and has an important connection with semi-parametric efficiency theory. Derivation of the EIF is a crucial step in the development of asymptotically efficient estimators, yet mathematical techniques for going about this are not straightforward, often requiring specialized calculations. Helpfully, though, there has been some significant effort recently to distill the key points and steps, with reference to concepts familiar from elementary calculus (e.g., partial derivatives, chain rule, product rule). We anticipate that success in this assignment will require reviewing resources external to course lectures (some given below), and we encourage you to work together to understand the major ideas and to make liberal use of tutorial opportunities in lab sessions and office hours. In particular, please consult Hines et al. (2022) and Kennedy (2022); you may also find online references like https://achambaz.github.io/tlride and https://alejandroschuler.github.io/mci to be helpful to varying degrees.

\newpage

# Deriving the EIF of the Covariance

Derive the efficient influence function (EIF) for the covariance of $A$ and $Y$ with respect to a data distribution $\mathrm{P} \in \mathcal{M}$, that is,

$$
\Psi(\mathrm{P})=\operatorname{Cov}_{\mathrm{P}}(A, Y)=\mathbb{E}_{\mathrm{P}}\left[\left(Y-\mathbb{E}_{\mathrm{P}}(Y)\right)\left(A-\mathbb{E}_{\mathrm{P}}(A)\right)\right] .
$$

Assume that the study unit is the $\operatorname{RV} O=(A, Y)$, where $O \sim \mathrm{P}_{0} \in \mathcal{M}$ and you have access to $O_{1}, \ldots, O_{n} \stackrel{\text { iid }}{\sim} \mathrm{P}_{n}$, where $\mathrm{P}_{0}$ is the true underlying (and unknown) data distribution (known only to lie within some broad statistical model $\mathcal{M}$ ) and $\mathrm{P}_{n}$ the empirical distribution of the sample.

*Hint: Let $O=(X, Y)$, and consider the following helpful identity:*

$$
\begin{aligned}
\left.\frac{d}{d t} \mathbb{E}_{\mathrm{P}_{t}}\left[g\left(O, \mathrm{P}_{t}\right)\right]\right|_{t=0} & =\left.\frac{d}{d t}\left[\int g\left(o, \mathrm{P}_{t}\right) f_{t}(o) d o\right]\right|_{t=0} \\
& =\left.\left\{\int \frac{d}{d t} g\left(o, \mathrm{P}_{t}\right) f_{t}(o) d o+\int g\left(o, \mathrm{P}_{t}\right) \frac{d}{d t} f_{t}(o) d o\right\}\right|_{t=0} \\
& =\left.\mathbb{E}_{\mathrm{P}}\left[\frac{d}{d t} g\left(o, \mathrm{P}_{t}\right)\right]\right|_{t=0}+g(\tilde{o}, \mathrm{P})-\mathbb{E}_{\mathrm{P}}[g(O, \mathrm{P})] .
\end{aligned}
$$
&nbsp;

Note: In the identity above, $\mathrm{P}_{t}$ is a parametric submodel (though the model $\mathcal{M}$ ) indexed by $t \in[0,1]$, so that $\mathrm{P}_{t}=t \tilde{\mathrm{P}}+(1-t) \mathrm{P}$, where $\mathrm{P}_{t=0} \equiv \mathrm{P}_{0}$, the underlying (and unknown) data distribution.

:::{.callout-note title="Answer"}

We will follow the example from @hines2022demystifying as follows:

### Step 1: Define the estimand of interest

$$
\Psi(\mathrm{P})=\operatorname{Cov}_{\mathrm{P}}(A, Y)=\mathbb{E}_{\mathrm{P}}\left[\left\{Y-\mathbb{E}_{\mathrm{P}}(Y)\right\}\left\{A-\mathbb{E}_{\mathrm{P}}(A)\right\}\right] 
$$

### Step 2: Calculate the estimand's efficient influence function

$$
\begin{aligned}
\left.\frac{d \Psi\left(\mathrm{P}\right)}{d t}\right|_{t=0}
&= \mathbb{E}_{\mathrm{P}}\left[\left.\frac{d}{d t}\left\{Y-\mathbb{E}_{\mathrm{P}_t}(Y)\right\}\left\{A-\mathbb{E}_{\mathrm{P}_t}(A)\right\}\right|_{t=0}\right]  +\left\{\tilde{y}-\mathbb{E}_{\mathrm{P}}(Y)\right\}\left\{\tilde{a}-\mathbb{E}_{\mathrm{P}}(A)\right\}-\Psi(\mathrm{P}) \quad (\textbf{Apply Identity 1}) \\ \\
&= \mathbb{E}_{\mathrm{P}}\left[-\left.\frac{d}{d t} \mathbb{E}_{\mathrm{P}_t}(Y)\right|_{t=0}\left\{A-\mathbb{E}_{\mathrm{P}}(A)\right\}-\left.\frac{d}{d t} \mathbb{E}_{\mathrm{P}_t}(A)\right|_{t=0}\left\{Y-\mathbb{E}_{\mathrm{P}}(Y)\right\} \right] \\ &\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad +\left\{\tilde{y}-\mathbb{E}_{\mathrm{P}}(Y)\right\}\left\{\tilde{a}-\mathbb{E}_{\mathrm{P}}(A)\right\}-\Psi(\mathrm{P}) \quad (\textbf{Chain Rule})\\ \\
&= \underbrace{\mathbb{E}_{\mathrm{P}}\left[-\left\{\tilde{y}-\mathbb{E}_{\mathrm{P}}(Y)\right\}\left\{A-\mathbb{E}_{\mathrm{P}}(A)\right\}-\left\{\tilde{a}-\mathbb{E}_{\mathrm{P}}(A)\right\}\left\{Y-\mathbb{E}_{\mathrm{P}}(Y)\right\} \right]}_{0 \text{ by  }(\star)}  \\ &\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad+\left\{\tilde{y}-\mathbb{E}_{\mathrm{P}}(Y)\right\}\left\{\tilde{a}-\mathbb{E}_{\mathrm{P}}(A)\right\}-\Psi(\mathrm{P}) \quad (\textbf{Apply Identity 1})\\ \\
&\Rightarrow 
\phi_{\mathrm{IC}}(O, \mathrm{P})= \left\{Y-\mathbb{E}_{\mathrm{P}}(Y)\right\}\left\{A-\mathbb{E}_{\mathrm{P}}(A)\right\}  -\Psi(\mathrm{P})\quad   
\end{aligned}
$$
&nbsp;

$(\star)$ For arbitrary X, $\mathbb{E}_P\{X - \mathbb{E}_P(X)\} = \mathbb{E}_P(X) - \mathbb{E}_P(\mathbb{E}_P(X)) = \mathbb{E}_P(X) - \mathbb{E}_P(X) = 0$. Thus, $\mathbb{E}_P\{A - \mathbb{E}_P(A)\}=\mathbb{E}_P\{Y - \mathbb{E}_P(Y)\}=0$.

:::

{{< pagebreak >}}

# Deriving the EIF of the Expected Conditional Covariance

Derive the efficient influence function for the expected conditional covariance of $A$ and $Y$, controlling for $L$, with respect to a data distribution $\mathrm{P} \in \mathcal{M}$, that is,

$$
\Psi(\mathrm{P})=\mathbb{E}\left[\operatorname{Cov}_{\mathrm{P}}(Y, A \mid L)\right]=\mathbb{E}_{\mathrm{P}}\left[\left(Y-\mathbb{E}_{\mathrm{P}}(Y \mid L)\right)\left(A-\mathbb{E}_{\mathrm{P}}(A \mid L)\right)\right]
$$

Assume that the study unit is the $\operatorname{RV} O=(L, A, Y)$, where $O \sim \mathrm{P}_{0} \in \mathcal{M}$ and you have access to $O_{1}, \ldots, O_{n} \stackrel{\text { iid }}{\sim} \mathrm{P}_{n}$, where $\mathrm{P}_{0}$ is the true underlying (and unknown) data distribution (known only to lie within some broad statistical model $\mathcal{M}$ ) and $\mathrm{P}_{n}$ the empirical distribution of the sample.

*Hints: Review Hines et al. (2022) in detail and see Example 4 from Kennedy (2022); also, the following identity, a form of which appears in Hines et al. (2022), will be helpful*

$$
\begin{aligned}
\left.\frac{d}{d t} \mathbb{E}_{\mathrm{P}_{t}}[g(O, \mathrm{P}) \mid X=x]\right|_{t=0} & =\left.\frac{d}{d t}\left[\int g\left(o, \mathrm{P}_{t}\right) f_{t}(y \mid x) d y\right]\right|_{t=0} \\
& =\frac{\mathbb{I}_{\tilde{x}}(x)}{f(x)}\left[g(\tilde{o}, \mathrm{P})-\mathbb{E}_{\mathbf{P}}\{g(O, P) \mid X=x\}\right]+\mathbb{E}_{\mathbf{P}}\left[\frac{d}{d t} g\left(O, \mathrm{P}_{t}\right)|X=x|_{t=0}\right],
\end{aligned}
$$

*where $O=(X, Y)$ in the identity above.*

:::{.callout-note title="Answer"}

We will follow the example from @hines2022demystifying as follows:

### Step 1: Define the estimand of interest

$$
\Psi(\mathrm{P})=\mathbb{E}\left[\operatorname{Cov}_{\mathrm{P}}(Y, A \mid L)\right]=\mathbb{E}_{\mathrm{P}}\left[\left\{Y-\mathbb{E}_{\mathrm{P}}(Y \mid L)\right\}\left\{A-\mathbb{E}_{\mathrm{P}}(A \mid L)\right\}\right]
$$

### Step 2: Calculate the estimand's efificent influence function

$$
\begin{aligned}
\left.\frac{d \Psi(\mathrm{P})}{d t}\right|_{t=0}&=\mathbb{E}_{\mathrm{P}}\left\{\frac{d}{d t} \operatorname{Cov}_t(Y, A \mid L)\right\}+\operatorname{Cov}(Y, A \mid \tilde{l})-\Psi(\mathrm{P})\quad (\textbf{Apply Identity 1}) \\ \\
&=\mathbb{E}_{\mathrm{P}}\left\{\frac{\mathbb{I}_{\tilde{l}}(l)}{f(l)}\left[\left\{\tilde{y}-\mathbb{E}_{\mathrm{P}}(Y \mid \tilde{l})\right\}\left\{\tilde{a}-\mathbb{E}_{\mathrm{P}}(A \mid \tilde{l})\right\}-\operatorname{Cov}(Y, A \mid L=l)\right]\right.\\ &\quad\quad\quad\quad\quad\quad\left.+\mathbb{E}_{\mathrm{P}}\left[\frac{d}{d t}\left\{Y-\mathbb{E}_{\mathrm{P}_t}(Y \mid L)\right\}\left\{A-\mathbb{E}_{\mathrm{P}_{\mathrm{t}}}(A \mid L)\right\} \mid L=l  \biggr\rvert_{t = 0}\right]\right\} \\ 
& \quad\quad\quad\quad\quad\quad+\operatorname{Cov}(Y, A \mid \tilde{l})-\Psi(\mathrm{P})\quad (\textbf{Apply Identity 2})
\\ \\
&=\mathbb{E}_{\mathrm{P}}\left\{\frac{\mathbb{I}_{\tilde{l}}(l)}{f(l)}\left[\left\{\tilde{y}-\mathbb{E}_{\mathrm{P}}(Y \mid \tilde{l})\right\}\left\{\tilde{a}-\mathbb{E}_{\mathrm{P}}(A \mid \tilde{l})\right\}-\operatorname{Cov}(Y, A \mid L=l)\right]\right.\\ &\quad\quad\quad\quad\quad\quad\left.+\mathbb{E}_{\mathrm{P}}\left[-\left.\frac{d}{d t} \mathbb{E}_{\mathrm{P}_t}(Y|L)\right|_{t=0}\left\{A-\mathbb{E}_{\mathrm{P}}(A|L)\right\}-\left.\frac{d}{d t} \mathbb{E}_{\mathrm{P}_t}(A|L)\right|_{t=0}\left\{Y-\mathbb{E}_{\mathrm{P}}(Y|L)\right\}\right]\right\} \\ 
& \quad\quad\quad\quad\quad\quad+\operatorname{Cov}(Y, A \mid \tilde{l})-\Psi(\mathrm{P}) \quad (\textbf{Chain Rule})
\\ \\
&=\underbrace{\mathbb{E}_{\mathrm{P}}\left\{\frac{\mathbb{I}_{\tilde{l}}(l)}{f(l)}\left[\left\{\tilde{y}-\mathbb{E}_{\mathrm{P}}(Y \mid \tilde{l})\right\}\left\{\tilde{a}-\mathbb{E}_{\mathrm{P}}(A \mid \tilde{l})\right\}-\operatorname{Cov}(Y, A \mid L=l)\right]\right.}_{0 \text{ by }\star \star} \\ &\quad\quad\quad\quad\quad\quad\left.+ \underbrace{\mathbb{E}_{\mathrm{P}}\left[-\left\{\tilde{y}-\mathbb{E}_{\mathrm{P}}(Y|L)\right\}\left\{A-\mathbb{E}_{\mathrm{P}}(A|L)\right\}-\left\{\tilde{a}-\mathbb{E}_{\mathrm{P}}(A|L)\right\}\left\{Y-\mathbb{E}_{\mathrm{P}}(Y|L)\right\}\right]}_{0 \text{ by } \star}\right\} \\ 
& \quad\quad\quad\quad\quad\quad+\operatorname{Cov}(Y, A \mid \tilde{l})-\Psi(\mathrm{P})\\ \\
&\Rightarrow 
\phi_{\mathrm{IC}}(O, \mathrm{P})= \left\{Y-\mathbb{E}_{\mathrm{P}}(Y \mid L)\right\}\left\{A-\mathbb{E}_{\mathrm{P}}(A \mid L)\right\}-\Psi(\mathrm{P})   
\end{aligned}
$$

&nbsp;

($\star \star$) The first expression in the second-to-last line evaluates to zero because the indicator function $\mathbb{I}_{\tilde{l}}(l)$ selects the case when $L = \tilde{l}$, but for all other values of $L$, the product is zero. Thus, when $L = \tilde{l}$, the expected value of the product of deviations is exactly the conditional covariance, by definition, and subtracting the conditional covariance from itself gives zero.

:::

{{< pagebreak >}}

# The One-Step Estimator

Derive the one-step estimator for the conditional covariance based on the expression for the canonical gradient of the expected conditional covariance obtained in the previous question. Start with the efficient influence function (in the nonparametric model $\mathcal{M}$) for the expected conditional covariance defined as

$$
\phi_{\mathrm{IC}}(O, \mathrm{P})=\left.\frac{d}{d t} \Psi\left(\mathrm{P}_{t}\right)\right|_{t=0}=\left(Y-\mathbb{E}_{\mathrm{P}}(Y \mid L)\right)\left(A-\mathbb{E}_{\mathrm{P}}(A \mid L)\right)-\Psi(\mathrm{P})
$$

where $\Psi(\mathrm{P})=\mathbb{E}[\operatorname{Cov}(Y, A \mid L)]$ and $\operatorname{Cov}(Y, A \mid L)=\mathbb{E}\{[Y-\mathbb{E}(Y \mid L)][A-\mathbb{E}(A \mid L)] \mid L\}$. The derivation of the influence function is given in Hines et al. (2022).

:::{.callout-note title="Answer"}

We start with the estimand and EIF derived above:

$$
\Psi(\mathrm{P}) = \mathbb{E}[\text{Cov}(Y, A \mid L)]
$$

$$
\phi_{IC}(O, \mathrm{P}) = \left(Y - \mathbb{E}_{\mathrm{P}}(Y \mid L)\right)\left(A - \mathbb{E}_{\mathrm{P}}(A \mid L)\right) - \Psi(\mathrm{P})
$$

Next, we can compute initial estimates for the conditional expectations $\hat{m}(l)$ and $\hat{\pi}(l)$ based on the observed data:

$$
\hat{m}(l) = \mathbb{E}_{\hat{\mathrm{P}}_n}(Y \mid l)
$$
$$
\hat{\pi}(l) = \mathbb{E}_{\hat{\mathrm{P}}_n}(A \mid l)
$$

We then use these models to compute the initial plug-in estimator for the expected conditional covariance $\Psi(\hat{P}_n)$:

$$
\hat{\Psi}(\hat{\mathrm{P}}_n) = \frac{1}{n} \sum_{i=1}^{n} \left\{Y_i - \hat{m}(L_i)\right\}\left\{A_i - \hat{\pi}(L_i)\right\}
$$

Then for each observation in the sample, compute the EIF using the initial estimates:

$$
\phi(O_i, \hat{\mathrm{P}}_n) = \left(Y_i - \hat{m}(L_i)\right)\left(A_i - \hat{\pi}(L_i)\right) - \hat{\Psi}(\hat{\mathrm{P}}_n)
$$

Finally, we adjust the initial estimate with the average of the EIFs to obtain the one-step estimator $\hat{\psi}_n^{OS}$:

$$
\hat{\psi}_n^{OS} = \hat{\Psi}(\hat{\mathrm{P}}_n) + \frac{1}{n} \sum_{i=1}^n \phi(O_i, \hat{\mathrm{P}}_n) = \frac{1}{n} \sum_{i=1}^n\left\{Y_i - \hat{m}(L_i)\right\}\left\{A_i - \hat{\pi}(L_i)\right\}
$$

<!--
Recall from lecture: The one-step estimator uses the original estimator as a starting point and adjusts it by the average of the EIF, which corrects for potential biases and utilizes the complete data information, leading to an asymptotically efficient estimator.
-->

:::

{{< pagebreak >}}

Based on this expression for the one-step estimator $\hat{\psi}_{n}^{\mathrm{OS}}$, implement the one-step (OS) estimator and explore its asymptotic properties in a simulation experiment. To do so, construct a data-generating process (DGP), and, using this DGP, simulate data at sample sizes $n=\{100,400,900,1600,2500\}$, setting a random seed to initialize the PRNG.

## a. Implement the OS estimator to obtain an estimate of the expected conditional covariance in each sample, using an appropriate parametric model (e.g., GLM) to estimate any relevant nuisance parameters. Visualize the sampling distributions of the OS estimator at each sample size and report relevant summary statistics.

:::{.callout-note title="Answer"}

```{R, cache=T, message=F, warning=F}

# rm(list = ls())

library(ggplot2)
library(mgcv)
library(dplyr)
library(tidyr)
library(broom)

set.seed(2024)

# Generate data
simulate_data <- function(n) {
  L1 <- rnorm(n)
  L2 <- runif(n, 0, 1)
  L3 <- rbinom(n, 1, 0.5)
  L4 <- rpois(n, 2)
  L5 <- rexp(n, 1)
  L6 <- rnorm(n, mean = L1 * 0.5)
  L7 <- rbinom(n, 1, L2)
  L8 <- ifelse(L3 == 1, rnorm(n, 1), rnorm(n, -1))
  L9 <- rlogis(n)
  L10 <- abs(rnorm(n)) + L4 * 0.1

  prob_A <- exp(0.25 * L1 + 0.5 * L2 - 0.3 * L5) / (1 + exp(0.25 * L1 + 0.5 * L2 - 0.3 * L5))
  A <- rbinom(n, 1, prob_A)

  Y <- 2 * A + L1 - L2 * 0.5 + L5 + L6 * 0.1 + rnorm(n)

  return(data.frame(L1, L2, L3, L4, L5, L6, L7, L8, L9, L10, A, Y))
}

# Function to calculate one-step estimator
one_step_estimator <- function(data) {
  fit_Y <- glm(Y ~ L1 + L2 + L5 + L6 + L8 + L9 + L10, data=data, family=gaussian())
  fit_A <- glm(A ~ L1 + L2 + L5 + L6 + L8 + L9 + L10, data=data, family=binomial())
  
  m_hat <- predict(fit_Y, newdata=data, type="response")
  pi_hat <- predict(fit_A, newdata=data, type="response")
  
  psi_hat_OS <- mean((data$Y - m_hat) * (data$A - pi_hat))
  return(psi_hat_OS)
}

# Approximate true value Psi 
large_data <- simulate_data(100000)
true_psi <- one_step_estimator(large_data)

# Set simulation parameters
n_sizes <- c(100, 400, 900, 1600, 2500)
n_simulations <- 100

# Store results here
results <- data.frame(sample_size=rep(n_sizes, each=n_simulations), estimate=NA, bias=NA)

# Perform simulations
for (i in 1:nrow(results)) {
  data <- simulate_data(results$sample_size[i])
  results$estimate[i] <- one_step_estimator(data)
  results$bias[i] <- results$estimate[i] - true_psi
}

```

```{R, cache=T, message=F, warning=F}

# Visualize the sampling distributions
ggplot(results, aes(x=estimate, fill=factor(sample_size))) +
  geom_density(alpha=0.6) +
  theme_minimal() +
  labs(title="Sampling Distribution of the One-Step Estimator using GLM",
       x="Estimate", y="Density", fill="Sample Size") +
  scale_x_continuous(limits = c(0, .8), breaks = seq(0, .8, by = .1))

# Report summary statistics for each sample size
summary_stats_glm <- results %>%
  group_by(sample_size) %>%
  summarise(
    mean_estimate = mean(estimate),
    sd = sd(estimate),
    var = var(estimate),
    mean_bias = mean(bias)
  )

print(summary_stats_glm)

```


:::

{{< pagebreak >}}

## b. Repeat (a) above, instead implementing a nonparametric regression approach or a super learner (van der Laan, Polley, and Hubbard 2007) to estimate any nuisance parameters.

:::{.callout-note title="Answer"}

```{R, cache=T, message=F, warning=F}

set.seed(2024)

# Function to calculate one-step estimator using GAM
one_step_estimator_gam <- function(data) {
  fit_Y <- gam(Y ~ s(L1) + s(L2) +  s(L5) + s(L6) + s(L8) + s(L9) + s(L10), data=data)
  fit_A <- gam(A ~ s(L1) + s(L2) +  s(L5) + s(L6) + s(L8) + s(L9) + s(L10), data=data, family=binomial)

  m_hat <- predict(fit_Y, newdata = data, type = "response")
  pi_hat <- predict(fit_A, newdata = data, type = "response")

  psi_hat_OS <- mean((data$Y - m_hat) * (data$A - pi_hat))
  return(psi_hat_OS)
}

# Approximate true value Psi 
true_psi_gam <- one_step_estimator_gam(large_data)

# Set simulation parameters
n_sizes <- c(100, 400, 900, 1600, 2500)
n_simulations <- 100

results <- data.frame(sample_size = rep(n_sizes, each = n_simulations), estimate = NA, bias = NA)

# Perform simulations using the GAM-based function
for (i in 1:nrow(results)) {
  data <- simulate_data(results$sample_size[i])
  results$estimate[i] <- one_step_estimator_gam(data)
  results$bias[i] <- results$estimate[i] - true_psi_gam
}

```

```{R, cache=T, message=F, warning=F}

# Visualize the sampling distributions
ggplot(results, aes(x = estimate, fill = factor(sample_size))) +
  geom_density(alpha = 0.6) +
  theme_minimal() +
  labs(title = "Sampling Distribution of the One-Step Estimator using GAM",
       x = "Estimate", y = "Density", fill = "Sample Size") +
  scale_x_continuous(limits = c(0, .8), breaks = seq(0, .8, by = .1))

# Report summary statistics for each sample size
summary_stats_gam <- results %>%
  group_by(sample_size) %>%
  summarise(
    mean_estimate = mean(estimate),
    sd = sd(estimate),
    var = var(estimate),
    mean_bias = mean(bias)
  )

print(summary_stats_gam)


```


:::

{{< pagebreak >}}

## c. Are there notable differences between the sampling distributions of the OS estimator between parts (a) and (b)? If so, what might explain these?

:::{.callout-note title="Answer"}

The sampling distributions of the one-step (OS) estimator from the parametric Generalized Linear Model (GLM) and the nonparametric Generalized Additive Model (GAM) are shown above. In smaller samples, the OS estimator from the parametric GLM yields more precise estimates, which we expect due to the better stability of parametric models when data is limited. As the sample size grows, the differences between the sampling distributions diminish, with biases in both models converging towards zero. In general, this is what we expect in both cases for a correctly specified model.

&nbsp;

```{R}
# Merge GLM and GAM summary statistics
merged_stats <- merge(summary_stats_glm, summary_stats_gam, by="sample_size", suffixes = c("_glm", "_gam"))

# Calculate relative efficiency as the ratio of variances GLM/GAM (RE >1 indicates nonparametric has better efficiency)
merged_stats$relative_efficiency <- merged_stats$var_glm / merged_stats$var_gam

# Print relative efficiency
relative_efficiency_table <- merged_stats[, c("sample_size", "relative_efficiency")]
relative_efficiency_table

```

:::

{{< pagebreak >}}


# Variance-Weighted Treatment Effect

Are you curious about the significance of the expected conditional covariance? (This is a course in causal inference, after all.) This parameter is, in fact, directly related to the variance-weighted treatment effect (VTE). For a binary treatment $A \in\{0,1\}$, outcome $Y$ and covariates $L$, we have the following:


$$
\psi^{\mathrm{VTE}}=\Psi(\mathrm{P})=\frac{\mathbb{E}\{\operatorname{Cov}(Y, A \mid L)\}}{\mathbb{E}\{\mathbb{V}(A \mid L)\}}
=\mathbb{E}\left\{w(L) \mathbb{E}\left(Y^{1}-Y^{0} \mid L\right)\right\}
$$

where $w(L)=\mathbb{V}(A \mid L) / \mathbb{E}\{\mathbb{V}(A \mid L)\}$.

## Question: Demonstrate how the equation above is derived.

:::{.callout-note title="Answer"}

Given that $A$ is binary, we have:

$$
\mathbb{V}(A \mid L) = \mathbb{E}(A \mid L) \cdot (1 - \mathbb{E}(A \mid L))
$$

and

$$
\begin{aligned}
\operatorname{Cov}(Y, A \mid L) &= \mathbb{E}(A \cdot Y \mid L) - \mathbb{E}(A \mid L) \cdot \mathbb{E}(Y \mid L)\\
&= \mathbb{E}(Y \mid A=1, L) \cdot \mathbb{E}(A \mid L) - \mathbb{E}(A \mid L) \cdot \mathbb{E}(Y \mid L)\\
&= \mathbb{E}(A \mid L) \left\{ \mathbb{E}(Y \mid A=1, L) -  \mathbb{E}(Y \mid L) \right\} \\
&= \mathbb{E}(A \mid L) \left\{ \mathbb{E}(Y \mid A=1, L) - \left[ \mathbb{E}(Y \mid A=1, L) \cdot \mathbb{E}(A \mid L) + \mathbb{E}(Y \mid A=0, L) \cdot (1 - \mathbb{E}(A \mid L)) \right] \right\} \\
&= \mathbb{E}(A \mid L) \left\{ \mathbb{E}(Y \mid A=1, L) (1 - \mathbb{E}(A \mid L)) - \mathbb{E}(Y \mid A=0, L) (1 - \mathbb{E}(A \mid L)) \right\} \\
&= \mathbb{E}(A \mid L)(1 - \mathbb{E}(A \mid L)) \left[ \mathbb{E}(Y \mid A=1, L) - \mathbb{E}(Y \mid A=0, L) \right] \\
&= \mathbb{V}(A \mid L) \left[ \mathbb{E}(Y \mid A=1, L) - \mathbb{E}(Y \mid A=0, L) \right]
\end{aligned}
$$

Under consistency, we have:
$$
\mathbb{E}\left(Y^1-Y^0 \mid L\right)=\mathbb{E}(Y \mid A=1, L)-\mathbb{E}(Y \mid A=0, L)
$$

Substituting into the expression for $\psi^{\mathrm{VTE}}$, we have:
$$
\begin{aligned}
\psi^{\mathrm{VTE}} &= \frac{\mathbb{E}\{\operatorname{Cov}(Y, A \mid L)\}}{\mathbb{E}\{\mathbb{V}(A \mid L)\}} \\
&=\frac{\mathbb{E}\left\{\mathbb{V}(A \mid L) \left[ \mathbb{E}(Y \mid A=1, L) - \mathbb{E}(Y \mid A=0, L) \right]\right\}}{\mathbb{E}\{\mathbb{V}(A \mid L)\}}\quad \text{Substitute covariance expression.}\\
&=\mathbb{E}\left\{\frac{\mathbb{V}(A \mid L) }{\mathbb{E}\{\mathbb{V}(A \mid L)\}}\left[ \mathbb{E}(Y \mid A=1, L) - \mathbb{E}(Y \mid A=0, L) \right]\right\} \\
&=\mathbb{E}\left\{\frac{\mathbb{V}(A \mid L) }{\mathbb{E}\{\mathbb{V}(A \mid L)\}}\mathbb{E}\left(Y^1-Y^0 \mid L\right)\right\} \quad \text{Consistency.}\\
&=\mathbb{E}\left\{w(L)\mathbb{E}\left(Y^1-Y^0 \mid L\right)\right\} \quad \text{Definition of } w(L).
\end{aligned}
$$

:::

{{< pagebreak >}}

## Question: Derive the influence function for the variance-weighted treatment effect using only the influence functions for the expected conditional covariance.

*Hint: The expected conditional variance is a special case of the expected conditional covariance where $\mathbb{V}(A \mid L)=\operatorname{Cov}(A, A \mid L)$.*

:::{.callout-note title="Answer"}

### Step 1: Define the estimand of interest

$$
\psi^{\mathrm{VTE}}=\Psi(\mathrm{P})=
\frac{\mathbb{E}\{\operatorname{Cov}(Y, A \mid L)\}}{\mathbb{E}\{\mathbb{V}(A \mid L)\}}
=\frac{\mathbb{E}\{\operatorname{Cov}(Y, A \mid L)\}}{\mathbb{E}\{\operatorname{Cov}(A, A \mid L)\}}
$$

### Step 2: Calculate the estimand's efficient influence function

From above, we know the EIC for $\Psi(\mathrm{P})=\mathbb{E}\left[\operatorname{Cov}_{\mathrm{P}}(Y, A \mid L)\right]$ is:

$$
\phi_{\mathrm{IC}}(O, \mathrm{P})= \left\{Y-\mathbb{E}_{\mathrm{P}}(Y \mid L)\right\}\left\{A-\mathbb{E}_{\mathrm{P}}(A \mid L)\right\}-\Psi(\mathrm{P})  
$$

Then we have:

$$
\begin{aligned}
\left.\frac{d \Psi(\mathrm{P})}{d t}\right|_{t=0}
&=\frac{\left.\frac{d}{dt} \mathbb{E}\{\operatorname{Cov}(Y, A \mid L)\} \right|_{t=0}
\mathbb{E}\{\mathbb{V}(A \mid L)\}- \mathbb{E}\{\operatorname{Cov}(Y, A \mid L)\}\left.\frac{d}{dt}\mathbb{E}\{\mathbb{V}(A \mid L)\}  \right|_{t=0}}{\mathbb{E}\{\mathbb{V}(A \mid L)\}^2}\\
&= \frac{\phi_{\text{Cov}, \mathrm{IC}}(O, \mathrm{P})\mathbb{E}\{\mathbb{V}(A \mid L)\} - \mathbb{E}\{\operatorname{Cov}(Y, A \mid L)\}\phi_{\text{Var}, \mathrm{IC}}(O, \mathrm{P})}{\mathbb{E}\{\mathbb{V}(A \mid L)\}^2}
\end{aligned}
$$

Expanding the influence functions:

$$
\begin{aligned}
\phi_{\text{Cov}, \mathrm{IC}}(O, \mathrm{P}) 
&= \left(Y-\mathbb{E}_{\mathrm{P}}(Y \mid L)\right)\left(A-\mathbb{E}_{\mathrm{P}}(A \mid L)\right) - \mathbb{E}\{\operatorname{Cov}(Y, A \mid L)\} 
\\ \\
\phi_{\text{Var}, \mathrm{IC}}(O, \mathrm{P}) 
&=\left(A-\mathbb{E}_{\mathrm{P}}(A \mid L)\right)^2 - \mathbb{E}\{\mathbb{V}(A \mid L)\}  
\end{aligned}
$$

Substituting and simplifying:

$$
\begin{aligned}
\left.\frac{d \Psi(\mathrm{P})}{d t}\right|_{t=0}
&= \frac{\left[\left(Y-\mathbb{E}_{\mathrm{P}}(Y \mid L)\right)\left(A-\mathbb{E}_{\mathrm{P}}(A \mid L)\right) - \mathbb{E}\{\operatorname{Cov}(Y, A \mid L)\}\right]\mathbb{E}\{\mathbb{V}(A \mid L)\}}{\mathbb{E}\{\mathbb{V}(A \mid L)\}^2} \\
&\quad -\frac{\mathbb{E}\{\operatorname{Cov}(Y, A \mid L)\}\left[\left(A-\mathbb{E}_{\mathrm{P}}(A \mid L)\right)^2 - \mathbb{E}\{\mathbb{V}(A \mid L)\}\right]}{\mathbb{E}\{\mathbb{V}(A \mid L)\}^2} \\
&= \frac{\left(Y-\mathbb{E}_{\mathrm{P}}(Y \mid L)\right)\left(A-\mathbb{E}_{\mathrm{P}}(A \mid L)\right) \mathbb{E}\{\mathbb{V}(A \mid L)\} - \mathbb{E}\{\operatorname{Cov}(Y, A \mid L)\}\mathbb{E}\{\mathbb{V}(A \mid L)\} }{\mathbb{E}\{\mathbb{V}(A \mid L)\}^2} \\
&\quad - \frac{\mathbb{E}\{\operatorname{Cov}(Y, A \mid L)\}\left(\left(A-\mathbb{E}_{\mathrm{P}}(A \mid L)\right)^2 - \mathbb{E}\{\mathbb{V}(A \mid L)\}\right)}{\mathbb{E}\{\mathbb{V}(A \mid L)\}^2} \\
&= \frac{\left(Y-\mathbb{E}_{\mathrm{P}}(Y \mid L)\right)\left(A-\mathbb{E}_{\mathrm{P}}(A \mid L)\right) - \Psi(\mathrm{P})\left(A-\mathbb{E}_{\mathrm{P}}(A \mid L)\right)^2}{\mathbb{E}\{\mathbb{V}(A \mid L)\}} \\
\end{aligned}
$$


:::

{{< pagebreak >}}

## Question: Use the influence function to derive the one step estimator for the variance-weighted treatment effect.

:::{.callout-note title="Answer"}


We derived the one step estimator for the expected conditional covariance as follows:

$$
\hat{\psi}_{EC}^{OS} = \hat{\Psi}(\hat{\mathrm{P}}_n) + \frac{1}{n} \sum_{i=1}^n \phi(O_i, \hat{\mathrm{P}}_n) = \frac{1}{n} \sum_{i=1}^n\left\{Y_i - \hat{m}(L_i)\right\}\left\{A_i - \hat{\pi}(L_i)\right\}
$$

Where $\hat{m}(L_i)$ and $\hat{\pi}(L_i)$ represent the regression estimates of $\mathbb{E}(Y \mid L)$ and $\mathbb{E}(A \mid L)$, respectively.

&nbsp;

For the variance-weighted treatment effect (VTE), which is a ratio of expected conditional covariances, the one-step estimator can be similarly defined as the ratio of respective one-step estimators as discussed in @kennedy2022semiparametric:

$$
\hat{\psi}_{VTE}^{OS} = \frac{\frac{1}{n}\sum_{i=1}^n\left\{Y_i-\hat{m}\left(L_i\right)\right\}\left\{A_i-\hat{\pi}\left(L_i\right)\right\}}{\frac{1}{n}\sum_{i=1}^n\left\{A_i-\hat{\pi}\left(L_i\right)\right\}^2}
$$

:::

{{< pagebreak >}}

# Variance-Weighted Treatment Effect in the NHEFS Dataset

In this question, you will apply your derived one-step estimator to derive an estimate of the variance-weighted treatment effect between the treatment effect and the outcome within the National Health and Nutrition Examination Survey I Epidemiologic Follow-up Study (NHEFS) dataset. This dataset, which you may recall from a previous homework exercise, is instrumental in studying the impact of smoking cessation on weight gain. For comprehensive details on the NHEFS, including access to the datasets and documentation, please visit CDC's NHEFS webpage (http: //wwwn.cdc.gov/nchs/nhanes/nhefs/). Additionally, a curated subset of the NHEFS data has been made available on Canvas for your use in this exercise. The data set includes data from 1,566 individuals, aged 25 to 74 , who were smokers at baseline and were followed up approximately a decade later.

The dataset features the following nine baseline covariates $(L)$, deemed adequate for adjusting confounding in our analysis:
- $\operatorname{Sex}$ (0: male, 1: female)
- Age (in years)
- Race (0: white, 1: other)
- Education (categorized into 5 levels)
- Intensity of smoking (measured by the number of cigarettes per day)
- Duration of smoking (measured in years)
- Daily life physical activity (categorized into 3 levels)
- Recreational exercise (categorized into 3 levels)
- Weight (in kilograms)

## Question: Calculate the variance weighted treatment effect between the treatment variable $A$ (smoking cessation) and the outcome variable $Y$ (weight gain), given the covariates $L$ using the one step estimator and a plug-in estimator. You are free to choose any model for estimating the nuisance parameters involved in applying the one-step estimator. Please clearly justify your choice of model(s), including any assumptions made in their application.

:::{.callout-note title="Answer"}

### Nuisance Parameter Estimation

To estimate $\mathbb{E}[Y|L]$, we use a linear regression model that incorporates both linear and quadratic terms for continuous variables, utilizing the same set of covariates as those specified in Homework 2. For estimating $\mathbb{E}[A|L]$, we apply a generalized linear model with a logit link function. This model also includes both linear and quadratic terms for continuous covariates, aligning with the approach taken in Homework 2.

```{R}

# Import data
nhefs <- read.csv("data/nhefs.csv")

# Remove individuals with missing outcome data (n=1566)
nhefs <- nhefs[!is.na(nhefs$wt82_71), ]

# Estimate E(Y|L) and E(A|L) where Y = wt82_71 and A = qsmk
model_Y <- lm(wt82_71 ~ sex + age + I(age^2) + race + factor(education) + smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + factor(active) + factor(exercise) + wt71 + I(wt71^2), 
              data = nhefs)
model_A <- glm(qsmk ~ sex + age + I(age^2) + race + factor(education) + smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + factor(active) + factor(exercise) + wt71 + I(wt71^2), 
               family = binomial, data = nhefs)

# Calculate predictions
nhefs$predicted_Y <- predict(model_Y, newdata = nhefs)
nhefs$predicted_A <- predict(model_A, newdata = nhefs, type = "response")

```

### Plug-in Estimator

First, we compute the plug-in estimator.

```{R}

# Predict expected outcomes under treatment and control for each individual
model_Y_plus_qsmk <- lm(wt82_71 ~ qsmk + sex + age + I(age^2) + race + factor(education) + smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + factor(active) + factor(exercise) + wt71 + I(wt71^2), 
              data = nhefs)
              
nhefs$ma_hat <- predict(model_Y_plus_qsmk, newdata = nhefs)
nhefs$m1_hat <- predict(model_Y_plus_qsmk, newdata = transform(nhefs, qsmk = 1))
nhefs$m0_hat <- predict(model_Y_plus_qsmk, newdata = transform(nhefs, qsmk = 0))
  
# Extract and rename variables
dat_pred <- with(nhefs, data.frame(
    m1_hat = m1_hat,
    m0_hat = m0_hat,
    ma_hat = ma_hat,
    p = predicted_A, 
    Y = wt82_71,
    A = qsmk
))

# Compute plug-in estimate
plugin <- with(dat_pred, mean(m1_hat - m0_hat))
print(paste("Plug-in Estimator:", round(plugin,4)))

```

### One-step Estimator

Next, we compute the one-step estimator.

```{R}
# Estimate covariance and variance components
cov_component <- mean((nhefs$wt82_71 - nhefs$predicted_Y) * (nhefs$qsmk - nhefs$predicted_A))
var_component <- mean((nhefs$qsmk - nhefs$predicted_A)^2)

# Plug-in estimator for VTE
onestep <- cov_component / var_component
print(paste("One-Step Estimator:", round(onestep,4)))

```

:::

## Question: Analytically calculate the standard error of the one step estimator using the influence function, and compare to your estimate of the standard error using bootstrap methods. Comment on your results.

:::{.callout-note title="Answer"}

### Analytical SE

```{R}

# Calculate EIF
eif <- ( ((nhefs$wt82_71 - nhefs$predicted_Y) * (nhefs$qsmk - nhefs$predicted_A)) - ((cov_component/var_component) * (nhefs$qsmk - nhefs$predicted_A)^2)) / var_component

# Calculate variance of EIF
var_eif <- var(eif)

# Calculate analytical standard error
n <- nrow(nhefs)
std_error_analytical <- sqrt(var_eif / n)
print(paste("Analytical Standard Error:", round(std_error_analytical, 4)))

```

### Boostrap SE

```{R}

library(boot)

# Bootstrap statistic function
boot_vte <- function(data, indices) {
  # Sample the data
  boot_sample <- data[indices, ]
  
  # Predict values using the models (assuming models are pre-fit)
  boot_sample$predicted_Y <- predict(model_Y, newdata = boot_sample)
  boot_sample$predicted_A <- predict(model_A, newdata = boot_sample, type = "response")
  
  # Calculate components for the VTE
  cov_component <- mean((boot_sample$wt82_71 - boot_sample$predicted_Y) * (boot_sample$qsmk - boot_sample$predicted_A))
  var_component <- mean((boot_sample$qsmk - boot_sample$predicted_A)^2)
  
return(cov_component / var_component)
}

# Perform the bootstrap
set.seed(2024)  
boot_results <- boot(data = nhefs, statistic = boot_vte, R = 1000)

# Calculate bootstrap standard error
bootstrap_se <- sd(boot_results$t)
print(paste("Bootstrap Standard Error:", round(bootstrap_se, 4)))

```

### Comparison

For the outcome model (linear regression), we assume that the relationship between the covariates $L$ and the outcome $Y$ is linear and can be adequately captured through the inclusion of linear and quadratic terms for the covariates. For the treatment model (logistic regression), we assume that the log-odds of the probability of $A$ given $L$ is linearly related to the covariates. Both models assume that all relevant covariates are included and correctly modeled, and that the observations are independent of one another.

&nbsp;

We observe that the analytical SE and bootstrap SE are almost identical. When the stated model assumptions are met, we anticipate that both the analytical and bootstrap methods will provide similar estimates of SE, indicating that the variance based on the influence function performs as we would expect. 

:::

{{< pagebreak >}}

## References

::: {#refs}
:::

*This assignment received assistance from ChatGPT for debugging code and compiling* \LaTeX.


















